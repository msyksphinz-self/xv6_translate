第5章 スケジューリング
==================

どのようなオペレーティングシステムも、コンピュータが持っているプロセッサの数以上のプロセスを実行し、従ってプロセス間で時分割共有することが必要になる。
理想的には、ユーザプロセスからはこの共有は見えないようにするべきである。
共通のアプローチとしては、各プロセスが個々に仮想マシンを保持しているように見せ掛け、オペレーティングシステムが複数の仮想マシンを単一のプロセッサで時分割共有して実行することである。
本性ではxv6がどのようにして複数のプロセスをプロセッサ上で実行しているのかについて説明する。

# 多重化

xv6は各プロセッサがあるプロセスから他のプロセスに切り替えることで多重化を行うが、これには2つの状態がある。
最初に、xv6は、あるプロセスがデバイスやパイプI/Oの完了を待つために待ち状態になると、sleepとwakeupの2つのメカニズムにより切り替えを行うか、子供が終了するのを待つか、sleepシステムコールによって終了するのを待つ。
次に、xv6がユーザ命令を実行中に、定期的に強制的に切り替えを行う。
この多重化により、各プロセスは自分のCPUを持っているように見えるが、xv6がメモリアロケータとハードウェアページテーブルにより各プロセスの固有のメモリを持っているように見せ掛けているだけである。

多重化を実装するには、いくつか困難な点がある。
最初に、あるプロセスからどのようにして別のプロセスに切り替えるのか？
xv6はコンテキストスイッチングの標準的なメカニズムを利用している; しかしアイデアはシンプルで、実装はシステムにおいて最も不透明である。
2番目に、どのようにして透過的なコンテキストスイッチングを実現するのか？
xv6は標準的なタイマ割り込みハンドラによりコンテキストスイッチを駆動している。
3番目に、多くのCPUはプロセスを同時に切り替えており、従ってレースコンディションを避けるためにロックの機構も考える必要がある。
4番目に、プロセスが終了したときに、そのメモリと資源を開放しなければならないが、しかしそれを自分自身では実行できない。
何故ならば、(例えば)自分が利用しているのに自分のカーネルスタックを開放することはできない。
xv6はこの問題をなるべくシンプルな方法で解決しようとしているが、結果として得られるコードはトリッキーである。

xv6はプロセスが自分自身を調整することのできる方法を提供しなければならない。
例えば、親プロセスはその子プロセスが終了するまで待つか、他のプロセスがパイプへの書き込みを行うのを待たなければならない。
プロセスが、所望のイベントが発生しているかチェックするためにCPUを無駄に利用するよりも、xv6はCPUの利用を諦めてイベントが発生するまでは眠っておき、他のプロセスが最初のプロセスを起動したほうが良い。
イベントの通知を読み落とすことを避けるために、レースコンディションを避けるためのケアが必要になる。
この問題と解答の例として、本章ではパイプの実装について取り扱う。

# コード例: コンテキストスイッチング

図5-1に示すように、プロセス間で切り替えを行うためには、xv6は低レイヤにおいて2種類のコンテキストスイッチを行っている: プロセスのカーネルスレッドから現在のCPUのスケジューラスレッドへの切り替えと、スケジューラスレッドからプロセスのカーネルスレッドへの切り替えである。
xv6は、決してあるユーザ空間のプロセスから他のプロセスへ直接切り替えすることはない; ところで、この状況はユーザカーネルの変換(システムコールもしくは割り込み)によって発生することはあるが、スケジューラへのコンテキストスイッチ、新しいプロセスのカーネルスレッドへのコンテキストスイッチ、およびトラップにより帰るxxx。
本章ではこのメカニズムの説明として、カーネルスレッドとスケジューラスレッドを取り扱う。

第2章で見てきたように、全てのxv6のプロセスは自分自身のカーネルスタックとレジスタセットを持っている。
各CPUは任意のプロセスのカーネルスレッド向けではなく、スケジューラを実行するための、分離したスケジューラスレッドを持っている。
ある1つのスレッドから他のスレッドに切り替えるために、古いスレッドのCPUレジスタを対比し、新しいスレッドのレジスタを復帰させるという処理が発生する;
%espと%eipが保存と回復が実行され、CPUがスタックをスイッチして、実行しているコードもスイッチしていることを意味する。

swtchはスレッドのことを直接知っているわけではない;contextsと呼ばれるレジスタセットの保存と復帰を行う処理を実行しているだけである。
プロセスがCPUを使うことを諦めると、プロセスのカーネルスレッドがswtchを予備、自身のコンテキストを退避してスケジューラコンテキストへと飛ぶ。
各コンテキストはstruct context*として表現されており、関連するカーネルスタックの構造体のポインタとして表現されている。
swtchは2つの引数を取る; struct context \*\*old とstruct context \*new である。
swtchは現在のCPUレジスタをスタックに保存して、スタックのポインタを\*oldに保存する。
次に、swtchはnewを%espにコピーし、前の保存したレジスタをポップしてから関数から戻る。

swtch内を見てスケジューラを追いかける代わりに、私たちのユーザプロセスが復帰するところを見てみよう。
第3章において、各割り込みの最後にtrapがyieldを呼び出す可能性があることについて触れた。
yieldはschedを呼び出し、schedはproc->contextに入っている現在のコンテキストを保存してcpu->schedulerによって保存している過去のスケジューラコンテキストにスイッチする(2766行目)。

swtch(2952行目)はまずスタックから引数をロードして、それを%eaxと%edx(2959-2960行目)に格納する;
swtchはスタックポインタを変更して%espを通じてどこにもアクセスできなくなる前にこれを実行する必要がある。
次に、swtchはレジスタステートを歩Zん市、現在のスタック上にコンテキスト構造体を作成する。
呼び出し先が保存するレジスタは保存する必要がある; x86はebp,%ebx,%esi,%ebp,%espが対象である。
swtchは最初の4つのレジスタを明示的にプッシュする(2963-2966行目); 最後のレジスタは、\*oldにstruct context*を書き込むことによって暗黙的に保存される。
さらに、もう一つ重要なレジスタが存在する: プログラムカウンタ%eipはswtchを呼び出すcallにより保存され、%ebpのスタックの上に格納される。
古いコンテキストを保存することによって、swtchは新しいコンテキストをロードする準備が整う。
swtchはポインタを新しいコンテキストのスタックポインタに移す(2970行目)。
新しいスタックはswtchが保存した古いスタックのもとの構造的には一緒である - 新しいスタックは前のswtchが呼ばれたときは古いスタックだったのである - したがって、swtchは
新しいコンテキストを退避する手順を逆に踏んでいけばよい。
%edi,%esi,%ebx,%ebpをポップし、買えされた命令アドレスは新しいコンテキストのものである。

私たちの例では、schedはswtchを呼び出してcpu->schedulerにスイッチして、CPU毎のスケジューラコンテキストにスイッチする。
コンテキストはschedulerにより保存され、swtchが呼ばれる(2728行目)。
swtchがどこに戻るかをトレースして言ったとき、schedには戻らずにschedulerに戻る。
スタックポインタは現在のCPUのスケジューラタスクを指しており、initprocのカーネルスタックを指しているわけではない。


# コード例: スケジューリング

前章では、swtchの低レイヤの詳細について確認した; では、swtchを例に取り、あるプロセスからスケジューラに移り、さらにプロセスに移るための慣習について見ていこう。
CPUの使用を取り止めたいプロセスは、プロセステーブルロックであるptable.lockを取得し、現在保持している全てのロックをリリーすし、現在の状態(proc->state)を更新し、schedを呼ぶ。
yield(2772)はこの慣習に従い、sleep命令とexit命令を実行する。これらについては後に見ることにする。
schedはこれらの状態の二重チェックを行い(2757-2762行目)、これらの状態のimplication(xxx)を行う:何故ならば、ロクを獲得すると、CPUは割り込みを無効にして実行するべきだからである。
最後に、schedjhalswtchを呼び、proc->contextの現在のコンテキストを保存して、cpu->schedulerにより保持されているスケジューラコンテキストにスイッチする(2728行目)。
スケジューラはforループを実行し続け、実行できるプロセスを見つけ、スイッチングすることを続ける。

xv6がはswtchを呼び出している間、ptable.lockを保持するところを見た: swtchの呼び出し元は既にこのロックを獲得している必要があり、ロックの制御はコードのスイッチングに渡される。
この慣習はロックにとって通常のことではない; 典型的な慣習は、ロックを獲得したスレッドがロックの解放の責任を持つことであり、これは正しさを保証するためには当然のことである。
コンテキストスイッチングのためには、典型的な慣習を破壊する必要がある。
何故ならば、ptable.lockは、swtchを実行中には真ではないプロセスの状態とcontextフィールドの不変性を保護しているからである。
ptable.lockがswtchの間中保持されていなかった場合に発生する問題の例を示す: 異なるCPUが、yieldが状態をRUNNABLEに変更した後に、どのプロセスを実行するかを来める必要があるが、swtchを呼ぶ前にカーネルスタックを使うことを止める。
この結果により、同一のスタック上で実行している2つのCPUの実行状態を、正しく設定することができなくなる。

カーネルスレッドは、schedの中でいつもプロセッサの利用を止め、スケジューラ中の同一の場所にスイッチし、sched内で(殆ど)常にプロセスにスイッチする。
従って、もしxv6がスレッドをスイッチした行番号をプリントすると、以下のようなシンプルなパタンが存在するはずである(2728行目),(2766行目)、(2728行目)、(2766行目)である。
このような形式で2つのスレッドがスイッチングを発生させることを、「コルーチン」と読んでいる;この例では、schedとschedulerがそろぞれコルーチンである。

新しいプロセスがsched内で終了しない例がある。
第2章で見たように、新しいプロセスが最初にスケジュールされたときである。新しいプロセスは、forkret(2783行目)から実行を開始する。
forkretはptable.lockを解放することで、この慣習を守るための存在している; そうでなければ、新しいプロセスはtrapretからスタートすることになる。

scheduler(2708行目)は単純なループを実行する: 実行可能なプロセスを見つけ、それが停止するまで実行することを繰替えす。
schedulerは殆ど全ての動作中に、ptable.lockのロックを保持しているが、各繰り返しにおいて、ループの外に出るときだけロックを解放する(そして、明示的に割り込みを許可する)。
これは、CPUがアイドル状態のとき(RUNNABLEなプロセスを発見することができなかったとき)に重要である。
アイドル状態のスケジューラがロックを保持し続けていると、プロセスを実行している他のCPUがコンテキストスイッチや、システムコールに関連するプロセスを実行したり、さらに特にプロセスをRUNNABLEに設定する操作ができず、遊休状態のCPUが二度とスケジューリングできなくなってしまう。
定期的に割り込みを許可する理由は、アイドル中のCPUで、例えばシェルのようなI/O待ちの状態でRUNNABLEのプロセスが存在しない場合のためである;
もしスケジューラが割り込みを常に不許可にしていた場合、I/Oの割り込みはもう二度と発生しなくなってしまう。

スケジューラはテーブルを参照しながら、p->state==RUNNABLEであるプロセス、つまり実行可能な状態にあるプロセスを探し続ける。
プロセスをはっけん すると、CPU毎の現在のプロセスの変数であるprocを設定し、プロセスのページテーブルをswitchuvmによりスイッチし、プロセスがRUNNNIGであると設定し、swtchを実行してプロセスの実行を開始する(2722-2728行目)。

スケジューリングのコードの構造について考えるための一つの方法は、各プロセスが常に不変性を維持するように調整されているとして、その不変性が真でなくなるときは常にptable.lockが保持されていると考えることである。
不変性の一つは、もしプロセスがRUNNING状態であれば、実行状態は整っており、タイマー割り込みのyieldは正しくプロセスからスイッチすることができる; これは、CPUのレジスタがそのプロセスの値を帆いしており(例えば、それらは実際にはcontextの中には存在しない)、%cr3はプロセスのページテーブルを参照しており、%espはプロセスのカーネルスタックを参照してなければならず、従って、swtchはレジスタを正しくプッシュしており、procはプロセスのproc[]スロットを参照していなければならない。
他の不変性は、もしプロセスがRUNNABLEであれば、アイドル状態のCPUでは、スケジューラを実行することができる; p->contextはプロセスのカーネルスレッドの値を持っており、プロセスのカーネルスタックを実行しているCPUは存在せず、CPUの%cr3はプロセスのページテーブルを参照しておらず、CPUのprocはプロセスを参照してはいない。

上記の不変性を管理することが、xv6がptable.lockを1つのスレッド(しばしばyieldの中)で獲得し、異なるスレッド(スケーウラスレッドもしくは他の次のカーネルスレッド)で解放する理由である。
実行しているプロセスの状態をRUNNABLEに設定するための変更が始まると、その不変性が修正されるまでは、lockを保持していなければならない： 最短の正しい解放ポイントは、schedulerがプロセスのページテーブルを使用するのを止め、procをクリアするところである。
同様に、一度schedulerが実行状態のプロセスをRUNNINGに変更する場合は、カーネルスレッドが完全に実行する状態になるまで(swtchをを実行してから、例えばyieldの中で)ロックは解放することができない。

ptable.lockは同様に、他の部分についても保護を行っている: プロセスのIDの割り当てと、プロセスのテーブルの解放処理と、exitとwaitの相互作用と、wakeupのロストを避けるための手続き(次章を参照のこと)と、他にも様々なことに利用される。
ptable.lockの他の機能について考えることは、明確性については確実に、性能についてはおそらく、分割して考えることが価値のあることになるxxx。

# sleepとwakeup

スケジューリングとロックは、あるプロセスを他のプロセスから存在を隠すことを助けるが、今のところはプロセスが意図的に相互作用することを助けるための抽象化は存在していない。
sleepとwakeupはれを埋めるものであり、プロセスがイベントを待つためにスリープ状態に入り、イベントが発生すると他のプロセスが置きる、ということができるようになる。
sleepとwakeupは「sequence coordination」もしくは「conditional synchronization」のメカニズムと呼ばれ、オペレーティングシステムの文献には、他にも似たような多くのメカニズムが存在する。

この構造を説明するために、まずは簡単な生産者と消費者のキューを考える。
このキューハプロセスからコマンドも受けとるIDEのドライバと似ている(第3章を参照のこと)が、IDEの特定のコードからは抽象化されている。
キューはあるプロセスが非ゼロのポインタを他のプロセスに送信することを許可している。
もし送信者が1つで、受信者も1つであり、それらが異なるCPU上で動作していれば、コンパイラは強力に最適化をすることは無く、以下のような実装で実現することができる:
```
100 struct q {
101     void *ptr;
102 };
103
104 void*
105 send(struct q *q, void *p)
106 {
107     while(q->ptr != 0)
108     ;
109     q->ptr = p;
110 }
111
112 void*
113 recv(struct q *q)
114 {
115     void *p;
116
117     while((p = q->ptr) == 0)
118     ;
119     q->ptr = 0;
120     return p;
121 }
```

sendは、キューが空の間は実行し続け、ポインタpをキューに挿入する。
recvはキューが空でない間は実行し続け、ポインタを取り出す。
異なるプロセスとして実行されているときは、sendとrecvはどちらともq->ptrを変更するが、sendはq->ptrがゼロのときだけ書き込み、recvはp->ptrが非ゼロのときだけ書き込む。従って、更新情報をロストすることはない。

上記の実装はコストが高い。
もし送信者が殆ど送信をしなければ、受信者はwhileループの中でポインタがやって来るまでスピンしながら待っていなければならない。
受信者のCPUは、もし受信者がCPUを消費する他の方法が存在すれば、sendがポインタを送信するときだけ回復し、それ以外のときは眠っていられる。

以下のように動作する、sleepとwakeupの2つの呼び出しを想像してみよう。
sleep(chan)は、任意の値chan上でスリープ状態に入る。これをwaitチャネルと呼ぶ。
sleepはスリープ状態に入るためにプロセスを呼び出し、他の仕事のためにCPUを手放す。
wakeup(chan)はchan上でスリープ状態に入っている全てのプロセスを呼び出し(もし必要ならば)、戻るためにこれらのsleepを呼び出す(xxx)。
chan上でプロセスあ待っていなければ、wakeupは何もしない。
このようなsleepとwakeupを利用するために、以下のようにキューの実装を変更する。

```
201 void*
202 send(struct q *q, void *p)
203 {
204     while(q->ptr != 0)
205     ;
206     q->ptr = p;
207     wakeup(q); /* wake recv */
208 }
209
210 void*
211 recv(struct q *q)
212 {
213     void *p;
214
215     while((p = q->ptr) == 0)
216     sleep(q);
217     q->ptr = 0;
218     return p;
219 }
```

recvはスピン状態に入るのではなく、CPUを手放す。これは良い方法である。
しかし、このインタフェースで、図5-2で説明した「ロストしたwakeup」として知られている問題を解決するためのインタフェースを利用してsleepとwakeupを実装することは、簡単な話ではない。
例えば、recvが215行目のq->ptr==0であることを検出したとしよう。recvは215と216行目の間にいるとき、sendは他のCPU上で動作している:
sendはq->ptrを非ゼロの値に書き換え、wakeupを呼ぶが、スリープ状態に入っているプロセスは存在せず、何も起こらない。
recvは216行目を実行市、sleep()を実行することでスリープ状態に入る。
ここで問題が生じる: recvはスリープ状態に入り、ポインタを待っているが、それは既に到着している。
次のsendがrecvが起きてキュー上のポインタを消費するのを待つためにスリープ状態に入り、この時点でこのシステムではデッドロックが発生する。

この問題の原因は、recvはq->ptr==0が成立しなくなったときにのみスリープ状態に入り、それとは違うタイミングでsendを実行させるところにある。
以下のように、recvのコードを変更して不変性を保とうとするのは間違いである：

```
300 struct q {
301     struct spinlock lock;
302     void *ptr;
303 };
304
305 void*
306 send(struct q *q, void *p)
307 {
308     acquire(&q->lock);
309     while(q->ptr != 0)
310     ;
311     q->ptr = p;
312     wakeup(q);
313     release(&q->lock);
314 }
315
316 void*
317 recv(struct q *q)
318 {
319     void *p;
320
321     acquire(&q->lock);
322     while((p = q->ptr) == 0)
323         sleep(q);
324     q->ptr = 0;
325     release(&q->lock);
326     return p;
327 }
```

recvをこのようにして保護すると、ロックが322行目および323行目を実行されることからsendを防ぐため、wakeupがロストすることを回避できる。
しかしこれでもデッドロックが発生する: recvはスリープ状態に入っている間はロックを保持しており、ロックの解放を待つために送信者が永久に待ち続けることになる。

上記の方法を、ロックをsleepに渡すことにより、呼び出し元のプロセスがスリープ状態としてマークされ、スリープチャネルを待っている状態になってもロックを解放できるように変更する。
ロックは受信者が自分自身をスリープ状態にするまでsendが実行されるのを防ぎ、従って、wakeupはスリープしている受信者を確実に起こすことができる。
受信者がスリープ状態から起きると、関数から抜ける前に再びロックを獲得する。
最終的な、正しいコードは以下のようになる:

```
400 struct q {
401     struct spinlock lock;
402     void *ptr;
403 };
404
405 void*
406 send(struct q *q, void *p)
407 {
408     acquire(&q->lock);
409     while(q->ptr != 0)
410     ;
411     q->ptr = p;
412     wakeup(q);
413     release(&q->lock);
414 }
415
416 void*
417 recv(struct q *q)
418 {
419     void *p;
420
421     acquire(&q->lock);
422     while((p = q->ptr) == 0)
423         sleep(q, &q->lock);
424     q->ptr = 0;
425     release(&q->lock);
426     return p;
427 }
```

recvがq->lockを保持することによって、sendがrecvがq->ptrをチェックし、sleepを呼ぶ前に起きようとすることを防ぐ。
もちろん、受信側のプロセスはスリープ中はq->lockを解放しなければならず、従って送信者は起きることができる。
従って、q->lockをアトミックに解放、スリープ状態に入るために、受信者のプロセスを起こしてからスリープ状態に入ることができる。
